{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0xfILJetmz7"
      },
      "outputs": [],
      "source": [
        "# projeto desenvolvido no Google Colab\n",
        "!pip install fiona shapely pyproj rtree pygeos geopandas dask-geopandas import_ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJa_o9ALrOns",
        "outputId": "3f500f3d-6090-4b80-d062-12ac42ce2554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/lightnight\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import dask_geopandas as dgpd\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "\n",
        "from datetime import date, datetime, timedelta\n",
        "import time\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/lightnight/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG9PzvqXrQDn"
      },
      "outputs": [],
      "source": [
        "def city_geo():\n",
        "  ''' funcao que trata a base de dados de shapefile das regioes administrativas'''\n",
        "\n",
        "  geo_1 = gpd.read_file('resources/gadm_410-levels.gpkg', layer='ADM_1')\n",
        "  geo_2 = gpd.read_file('resources/gadm_410-levels.gpkg', layer='ADM_2')\n",
        "  \n",
        "  country_l2 = geo_2['GID_0'].unique()\n",
        "  \n",
        "  geo_1 = geo_1[~geo_1['GID_0'].isin(country_l2)]\n",
        "  geo_1 = geo_1[['GID_1', 'geometry']]\n",
        "  geo_1.columns = ['gid', 'geometry']\n",
        "  \n",
        "  geo_2 = geo_2[['GID_2', 'geometry']]\n",
        "  geo_2.columns = ['gid', 'geometry']\n",
        "\n",
        "  geo = pd.concat([geo_1, geo_2]).reset_index(drop=True)\n",
        "\n",
        "  return geo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYDpSIIvrQSG"
      },
      "outputs": [],
      "source": [
        "def extract_and_save(h5_path, _df_br, ano, mes, threshold=0):\n",
        "\n",
        "  '''funcao que extrai os dados de luminosidade, lat e long dos arquivos .h5 do NASA Black Marble \n",
        "  e faz o merge com a base de dados das regioes administrativas. '''\n",
        "    \n",
        "  h5 = h5py.File(h5_path, 'r')\n",
        "\n",
        "  lat = np.array(h5['/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/lat'])\n",
        "  lon = np.array(h5['/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/lon'])\n",
        "  \n",
        "  AllAngle_Composite_Snow_Free = np.array(h5['/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/AllAngle_Composite_Snow_Free'])\n",
        "  AllAngle_Composite_Snow_Covered = np.array(h5['/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/AllAngle_Composite_Snow_Covered'])\n",
        "\n",
        "  NearNadir_Composite_Snow_Covered = np.array(h5['/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/NearNadir_Composite_Snow_Covered'])\n",
        "  NearNadir_Composite_Snow_Free = np.array(h5['/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/NearNadir_Composite_Snow_Free'])\n",
        "\n",
        "  OffNadir_Composite_Snow_Covered = np.array(h5['/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/OffNadir_Composite_Snow_Covered'])\n",
        "  OffNadir_Composite_Snow_Free = np.array(h5['/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/OffNadir_Composite_Snow_Free'])\n",
        "\n",
        "  list_radiance = [AllAngle_Composite_Snow_Free, AllAngle_Composite_Snow_Covered,\n",
        "                   NearNadir_Composite_Snow_Covered, NearNadir_Composite_Snow_Free, \n",
        "                   OffNadir_Composite_Snow_Covered, OffNadir_Composite_Snow_Free]\n",
        "  \n",
        "  rad_name = ['AllAngle_Composite_Snow_Free', 'AllAngle_Composite_Snow_Covered',\n",
        "                   'NearNadir_Composite_Snow_Covered', 'NearNadir_Composite_Snow_Free', \n",
        "                   'OffNadir_Composite_Snow_Covered', 'OffNadir_Composite_Snow_Free']\n",
        "\n",
        "\n",
        "  d = {'latitude': [], 'longitude': [], 'radiance': [], 'radiance_type': []}\n",
        "  df = pd.DataFrame(data=d)\n",
        "\n",
        "  a = 0\n",
        "  for radiance in list_radiance:\n",
        "    df_ = pd.DataFrame(radiance, columns=lon, index=lat)\n",
        "    df_.replace(65535, np.nan, inplace=True)\n",
        "    df_ = df_.stack().reset_index()\n",
        "    df_.columns = ['latitude', 'longitude', 'radiance']\n",
        "    df_ = df_.dropna()\n",
        "    df_ = df_.loc[df_['radiance'] > threshold].reset_index(drop=True)\n",
        "    df_['radiance_type'] = rad_name[a]\n",
        "    df = pd.concat([df, df_])\n",
        "    a += 1\n",
        "  \n",
        "  # SJOIN utilizado para identificar se determinado pixel de luminosidade, com determinada latitutde e longitude esta contido dentro do multipolygon que representa determinada regiao\n",
        "  df = df.set_index(['latitude', 'longitude', 'radiance_type'])\n",
        "  df_light = df.unstack(level=-1)\n",
        "  df_light = df_light.reset_index(col_level=1)\n",
        "  df_light.columns = df_light.columns.droplevel(0)\n",
        "  \n",
        "  gdf_light = gpd.GeoDataFrame(df_light, geometry=gpd.points_from_xy(df_light.longitude, df_light.latitude))\n",
        "  gdf_light['geometry'] = gdf_light['geometry'].set_crs(epsg=4326)\n",
        "\n",
        "  df_merge = gpd.sjoin(gdf_light, _df_br, predicate='within')\n",
        "  df_merge['count_pixel'] = 1\n",
        "  df_merge = df_merge.drop(['index_right', 'latitude', 'longitude'], axis=1)\n",
        "\n",
        "  df_merge = df_merge.groupby(['gid']).sum()\n",
        "  df_merge = df_merge.reset_index()\n",
        "  df_merge['date'] = f'{ano}/{mes}'\n",
        "\n",
        "  if not df_merge.empty:\n",
        "    df_merge.to_csv('raw_dataset_3_world.csv', sep=';', index=False, mode='a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVwGOT7arQ0f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFzDW2yXrRBu",
        "outputId": "8724c07e-fc32-4e13-b40b-9f4b57285674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sucesso em coletar 2019/274\n"
          ]
        }
      ],
      "source": [
        "df_br = city_geo()\n",
        "\n",
        "# definir lista de anos  e meses (dia do ano) a serem processados\n",
        "anos = [2012, 2013, 2014]\n",
        "meses = []\n",
        "\n",
        "for ano in anos:\n",
        "  lista_meses = os.listdir(f'./VNP46A3/{ano}/')\n",
        "\n",
        "  for mes in meses:\n",
        "    lista_tiles = os.listdir(f'./VNP46A3/{ano}/{mes}')\n",
        "        \n",
        "    for tile in lista_tiles:\n",
        "      path = f'./VNP46A3/{ano}/{mes}/{tile}'\n",
        "      extract_and_save(path, df_br, ano, mes)\n",
        "    \n",
        "    print(f'sucesso em coletar {ano}/{mes}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}